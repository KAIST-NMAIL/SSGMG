{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSGMG",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dykim07/SSGMG/blob/master/SSGMG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGVmAJYVzAVe",
        "colab_type": "text"
      },
      "source": [
        "## Install library, data set, base codes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oX0wmWzM0n",
        "colab_type": "code",
        "outputId": "0cc791e5-5e92-437d-9cab-64a34763936a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# for colab\n",
        "!git clone https://github.com/dykim07/SSGMG.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SSGMG'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/66)   \u001b[K\rremote: Counting objects:   3% (2/66)   \u001b[K\rremote: Counting objects:   4% (3/66)   \u001b[K\rremote: Counting objects:   6% (4/66)   \u001b[K\rremote: Counting objects:   7% (5/66)   \u001b[K\rremote: Counting objects:   9% (6/66)   \u001b[K\rremote: Counting objects:  10% (7/66)   \u001b[K\rremote: Counting objects:  12% (8/66)   \u001b[K\rremote: Counting objects:  13% (9/66)   \u001b[K\rremote: Counting objects:  15% (10/66)   \u001b[K\rremote: Counting objects:  16% (11/66)   \u001b[K\rremote: Counting objects:  18% (12/66)   \u001b[K\rremote: Counting objects:  19% (13/66)   \u001b[K\rremote: Counting objects:  21% (14/66)   \u001b[K\rremote: Counting objects:  22% (15/66)   \u001b[K\rremote: Counting objects:  24% (16/66)   \u001b[K\rremote: Counting objects:  25% (17/66)   \u001b[K\rremote: Counting objects:  27% (18/66)   \u001b[K\rremote: Counting objects:  28% (19/66)   \u001b[K\rremote: Counting objects:  30% (20/66)   \u001b[K\rremote: Counting objects:  31% (21/66)   \u001b[K\rremote: Counting objects:  33% (22/66)   \u001b[K\rremote: Counting objects:  34% (23/66)   \u001b[K\rremote: Counting objects:  36% (24/66)   \u001b[K\rremote: Counting objects:  37% (25/66)   \u001b[K\rremote: Counting objects:  39% (26/66)   \u001b[K\rremote: Counting objects:  40% (27/66)   \u001b[K\rremote: Counting objects:  42% (28/66)   \u001b[K\rremote: Counting objects:  43% (29/66)   \u001b[K\rremote: Counting objects:  45% (30/66)   \u001b[K\rremote: Counting objects:  46% (31/66)   \u001b[K\rremote: Counting objects:  48% (32/66)   \u001b[K\rremote: Counting objects:  50% (33/66)   \u001b[K\rremote: Counting objects:  51% (34/66)   \u001b[K\rremote: Counting objects:  53% (35/66)   \u001b[K\rremote: Counting objects:  54% (36/66)   \u001b[K\rremote: Counting objects:  56% (37/66)   \u001b[K\rremote: Counting objects:  57% (38/66)   \u001b[K\rremote: Counting objects:  59% (39/66)   \u001b[K\rremote: Counting objects:  60% (40/66)   \u001b[K\rremote: Counting objects:  62% (41/66)   \u001b[K\rremote: Counting objects:  63% (42/66)   \u001b[K\rremote: Counting objects:  65% (43/66)   \u001b[K\rremote: Counting objects:  66% (44/66)   \u001b[K\rremote: Counting objects:  68% (45/66)   \u001b[K\rremote: Counting objects:  69% (46/66)   \u001b[K\rremote: Counting objects:  71% (47/66)   \u001b[K\rremote: Counting objects:  72% (48/66)   \u001b[K\rremote: Counting objects:  74% (49/66)   \u001b[K\rremote: Counting objects:  75% (50/66)   \u001b[K\rremote: Counting objects:  77% (51/66)   \u001b[K\rremote: Counting objects:  78% (52/66)   \u001b[K\rremote: Counting objects:  80% (53/66)   \u001b[K\rremote: Counting objects:  81% (54/66)   \u001b[K\rremote: Counting objects:  83% (55/66)   \u001b[K\rremote: Counting objects:  84% (56/66)   \u001b[K\rremote: Counting objects:  86% (57/66)   \u001b[K\rremote: Counting objects:  87% (58/66)   \u001b[K\rremote: Counting objects:  89% (59/66)   \u001b[K\rremote: Counting objects:  90% (60/66)   \u001b[K\rremote: Counting objects:  92% (61/66)   \u001b[K\rremote: Counting objects:  93% (62/66)   \u001b[K\rremote: Counting objects:  95% (63/66)   \u001b[K\rremote: Counting objects:  96% (64/66)   \u001b[K\rremote: Counting objects:  98% (65/66)   \u001b[K\rremote: Counting objects: 100% (66/66)   \u001b[K\rremote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/59)   \u001b[K\rremote: Compressing objects:   3% (2/59)   \u001b[K\rremote: Compressing objects:   5% (3/59)   \u001b[K\rremote: Compressing objects:   6% (4/59)   \u001b[K\rremote: Compressing objects:   8% (5/59)   \u001b[K\rremote: Compressing objects:  10% (6/59)   \u001b[K\rremote: Compressing objects:  11% (7/59)   \u001b[K\rremote: Compressing objects:  13% (8/59)   \u001b[K\rremote: Compressing objects:  15% (9/59)   \u001b[K\rremote: Compressing objects:  16% (10/59)   \u001b[K\rremote: Compressing objects:  18% (11/59)   \u001b[K\rremote: Compressing objects:  20% (12/59)   \u001b[K\rremote: Compressing objects:  22% (13/59)   \u001b[K\rremote: Compressing objects:  23% (14/59)   \u001b[K\rremote: Compressing objects:  25% (15/59)   \u001b[K\rremote: Compressing objects:  27% (16/59)   \u001b[K\rremote: Compressing objects:  28% (17/59)   \u001b[K\rremote: Compressing objects:  30% (18/59)   \u001b[K\rremote: Compressing objects:  32% (19/59)   \u001b[K\rremote: Compressing objects:  33% (20/59)   \u001b[K\rremote: Compressing objects:  35% (21/59)   \u001b[K\rremote: Compressing objects:  37% (22/59)   \u001b[K\rremote: Compressing objects:  38% (23/59)   \u001b[K\rremote: Compressing objects:  40% (24/59)   \u001b[K\rremote: Compressing objects:  42% (25/59)   \u001b[K\rremote: Compressing objects:  44% (26/59)   \u001b[K\rremote: Compressing objects:  45% (27/59)   \u001b[K\rremote: Compressing objects:  47% (28/59)   \u001b[K\rremote: Compressing objects:  49% (29/59)   \u001b[K\rremote: Compressing objects:  50% (30/59)   \u001b[K\rremote: Compressing objects:  52% (31/59)   \u001b[K\rremote: Compressing objects:  54% (32/59)   \u001b[K\rremote: Compressing objects:  55% (33/59)   \u001b[K\rremote: Compressing objects:  57% (34/59)   \u001b[K\rremote: Compressing objects:  59% (35/59)   \u001b[K\rremote: Compressing objects:  61% (36/59)   \u001b[K\rremote: Compressing objects:  62% (37/59)   \u001b[K\rremote: Compressing objects:  64% (38/59)   \u001b[K\rremote: Compressing objects:  66% (39/59)   \u001b[K\rremote: Compressing objects:  67% (40/59)   \u001b[K\rremote: Compressing objects:  69% (41/59)   \u001b[K\rremote: Compressing objects:  71% (42/59)   \u001b[K\rremote: Compressing objects:  72% (43/59)   \u001b[K\rremote: Compressing objects:  74% (44/59)   \u001b[K\rremote: Compressing objects:  76% (45/59)   \u001b[K\rremote: Compressing objects:  77% (46/59)   \u001b[K\rremote: Compressing objects:  79% (47/59)   \u001b[K\rremote: Compressing objects:  81% (48/59)   \u001b[K\rremote: Compressing objects:  83% (49/59)   \u001b[K\rremote: Compressing objects:  84% (50/59)   \u001b[K\rremote: Compressing objects:  86% (51/59)   \u001b[K\rremote: Compressing objects:  88% (52/59)   \u001b[K\rremote: Compressing objects:  89% (53/59)   \u001b[K\rremote: Compressing objects:  91% (54/59)   \u001b[K\rremote: Compressing objects:  93% (55/59)   \u001b[K\rremote: Compressing objects:  94% (56/59)   \u001b[K\rremote: Compressing objects:  96% (57/59)   \u001b[K\rremote: Compressing objects:  98% (58/59)   \u001b[K\rremote: Compressing objects: 100% (59/59)   \u001b[K\rremote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "Unpacking objects:   1% (1/66)   \rUnpacking objects:   3% (2/66)   \rUnpacking objects:   4% (3/66)   \rUnpacking objects:   6% (4/66)   \rUnpacking objects:   7% (5/66)   \rUnpacking objects:   9% (6/66)   \rUnpacking objects:  10% (7/66)   \rUnpacking objects:  12% (8/66)   \rUnpacking objects:  13% (9/66)   \rUnpacking objects:  15% (10/66)   \rUnpacking objects:  16% (11/66)   \rUnpacking objects:  18% (12/66)   \rUnpacking objects:  19% (13/66)   \rUnpacking objects:  21% (14/66)   \rUnpacking objects:  22% (15/66)   \rUnpacking objects:  24% (16/66)   \rUnpacking objects:  25% (17/66)   \rUnpacking objects:  27% (18/66)   \rUnpacking objects:  28% (19/66)   \rUnpacking objects:  30% (20/66)   \rUnpacking objects:  31% (21/66)   \rUnpacking objects:  33% (22/66)   \rUnpacking objects:  34% (23/66)   \rUnpacking objects:  36% (24/66)   \rUnpacking objects:  37% (25/66)   \rUnpacking objects:  39% (26/66)   \rUnpacking objects:  40% (27/66)   \rUnpacking objects:  42% (28/66)   \rUnpacking objects:  43% (29/66)   \rUnpacking objects:  45% (30/66)   \rUnpacking objects:  46% (31/66)   \rUnpacking objects:  48% (32/66)   \rUnpacking objects:  50% (33/66)   \rUnpacking objects:  51% (34/66)   \rUnpacking objects:  53% (35/66)   \rUnpacking objects:  54% (36/66)   \rUnpacking objects:  56% (37/66)   \rUnpacking objects:  57% (38/66)   \rremote: Total 66 (delta 19), reused 42 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects:  59% (39/66)   \rUnpacking objects:  60% (40/66)   \rUnpacking objects:  62% (41/66)   \rUnpacking objects:  63% (42/66)   \rUnpacking objects:  65% (43/66)   \rUnpacking objects:  66% (44/66)   \rUnpacking objects:  68% (45/66)   \rUnpacking objects:  69% (46/66)   \rUnpacking objects:  71% (47/66)   \rUnpacking objects:  72% (48/66)   \rUnpacking objects:  74% (49/66)   \rUnpacking objects:  75% (50/66)   \rUnpacking objects:  77% (51/66)   \rUnpacking objects:  78% (52/66)   \rUnpacking objects:  80% (53/66)   \rUnpacking objects:  81% (54/66)   \rUnpacking objects:  83% (55/66)   \rUnpacking objects:  84% (56/66)   \rUnpacking objects:  86% (57/66)   \rUnpacking objects:  87% (58/66)   \rUnpacking objects:  89% (59/66)   \rUnpacking objects:  90% (60/66)   \rUnpacking objects:  92% (61/66)   \rUnpacking objects:  93% (62/66)   \rUnpacking objects:  95% (63/66)   \rUnpacking objects:  96% (64/66)   \rUnpacking objects:  98% (65/66)   \rUnpacking objects: 100% (66/66)   \rUnpacking objects: 100% (66/66), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO93VZr_1k7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader as torchDataLoader\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OACt37OhzsPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for colab\n",
        "from SSGMG.dataLoader import dataLoader\n",
        "from SSGMG.models.AE import AE as PTMODEL\n",
        "from SSGMG.models.DFMNET import DFMNET as CALMODEL\n",
        "\n",
        "# for local host\n",
        "# from dataLoader import dataLoader\n",
        "# from models.AE import AE as PTMODEL\n",
        "# from models.DFMNET import DFMNET as CALMODEL\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS5ciZqezbxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# params\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "BATCH_SIZE_PT =  512\n",
        "LR_PT = 0.001\n",
        "N_EPOCHS_PT = 60\n",
        "N_LATENT_DIM = 3\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "LR = 0.001\n",
        "N_EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfJRIsb827Mr",
        "colab_type": "code",
        "outputId": "eaf354dc-0102-4b9f-cbbd-2373e7d0ea29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# load dataset\n",
        "path = os.path.join(os.getcwd(), 'SSGMG')\n",
        "print(path)\n",
        "dataloader = dataLoader(base_path = path)\n",
        "\n",
        "(train_x, train_y) = dataloader.getTrainDataSet()\n",
        "(test_x, test_y) = dataloader.getTestDataSet()\n",
        "pre_train_y = dataloader.getPretrainDataSet()\n",
        "\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "print(pre_train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SSGMG\n",
            "(2400, 120, 2) (2400, 30)\n",
            "(6000, 120, 2) (6000, 30)\n",
            "(6000, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDIZAr-9DMMU",
        "colab_type": "text"
      },
      "source": [
        "## Step.1 Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9gehgZmDPrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pt_y_torch = torch.from_numpy(pre_train_y).type(torch.float).to(DEVICE)\n",
        "valid_pt_y_torch = torch.from_numpy(test_y).type(torch.float).to(DEVICE)\n",
        "\n",
        "pt_model = PTMODEL(pre_train_y.shape[-1], N_LATENT_DIM)\n",
        "pt_model = pt_model.to(DEVICE)\n",
        "optimizer_pt = optim.Adam(pt_model.parameters(), lr = LR_PT)\n",
        "criterion_pt = nn.MSELoss()\n",
        "\n",
        "class DiabetesDatasetPT(Dataset):\n",
        "    def __init__(self, x_data):\n",
        "        self.len = x_data.size(0)\n",
        "        self.x_data = x_data\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index]\n",
        "   \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "\n",
        "torch_dataloader_pt = torchDataLoader(dataset=DiabetesDatasetPT(train_pt_y_torch),\n",
        "                                     batch_size = BATCH_SIZE_PT,\n",
        "                                     shuffle=True,\n",
        "                                     drop_last = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkaXF_CfHaH2",
        "colab_type": "code",
        "outputId": "e2b3d917-e1e5-4ccb-c0e5-eff7e27ff800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# train\n",
        "for epoch in tqdm(range(N_EPOCHS_PT)):\n",
        "    for idx, data in enumerate(torch_dataloader_pt):\n",
        "        pt_model.train()\n",
        "        optimizer_pt.zero_grad()\n",
        "        y_pred = pt_model(data)\n",
        "        loss_t = criterion_pt(y_pred, data)\n",
        "        loss_t.backward()\n",
        "        optimizer_pt.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [00:03<00:00, 17.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR0CUNGGH7ZB",
        "colab_type": "code",
        "outputId": "d6b134cf-7043-4c6a-b6ab-8949c7b11dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check error\n",
        "valid_y_pt = pt_model.transform(valid_pt_y_torch)\n",
        "recon = pt_model.inverse_transform(valid_y_pt).detach().to('cpu').numpy()\n",
        "rmse = np.sqrt(MSE(recon, test_y)) * 1000\n",
        "print(\"RECON error {:0.2f}\".format(rmse))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECON error 7.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9EnrXE2JUg4",
        "colab_type": "text"
      },
      "source": [
        "# Step.2 calibration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhs7HoW0J8kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data\n",
        "train_x_torch = torch.from_numpy(train_x).type(torch.float).to(DEVICE)\n",
        "train_y_pt_torch = torch.from_numpy(train_y).type(torch.float).to(DEVICE)\n",
        "train_y_pt_torch = pt_model.transform(train_y_pt_torch).detach()\n",
        "\n",
        "class DiabetesDataSet(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.len = x_data.size(0)\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "torch_dataloader_cal = torchDataLoader(dataset=DiabetesDataSet(train_x_torch,\n",
        "                                                              train_y_pt_torch),\n",
        "                                      batch_size=BATCH_SIZE,\n",
        "                                      shuffle=True,\n",
        "                                      drop_last=False)    \n",
        "    \n",
        "\n",
        "\n",
        "# model\n",
        "cal_model = CALMODEL(train_x_torch.size(-1), train_y_pt_torch.size(-1))\n",
        "cal_model = cal_model.to(DEVICE)\n",
        "criterion_cal = nn.MSELoss()\n",
        "optimizer_cal = optim.Adam(cal_model.parameters(), lr=LR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L8x1v8zkuAh",
        "colab_type": "code",
        "outputId": "11824ab8-b5fa-4a3d-9cb3-388d02a32c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for epoch in tqdm(range(N_EPOCHS)):\n",
        "    for x, y in torch_dataloader_cal :\n",
        "        optimizer_cal.zero_grad()\n",
        "        cal_model.train()\n",
        "        pred = cal_model(x)\n",
        "        loss_t = criterion_cal(pred, y)\n",
        "        loss_t.backward()\n",
        "        optimizer_cal.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:13<00:00,  2.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULbhWwUmmQgp",
        "colab_type": "text"
      },
      "source": [
        "# Step.3 PostProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7VckBi6lrMu",
        "colab_type": "code",
        "outputId": "52787e0b-02e0-402a-c9ca-8c4b19d62271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    test_x, test_y = dataloader.getTestDataSet()\n",
        "    test_x_torch = torch.from_numpy(test_x).type(torch.float).to(DEVICE)\n",
        "    cal_model.eval()\n",
        "    pred = cal_model(test_x_torch).detach()\n",
        "    pred = pt_model.inverse_transform(pred).detach().to('cpu').numpy()\n",
        "    rmse = np.sqrt(MSE(test_y, pred))*1000\n",
        "    print('RMSE: %0.2f' % rmse)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 25.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5-HEG_vm5zM",
        "colab_type": "code",
        "outputId": "9a3bfdd0-9a8c-4691-95e1-eab33ffad4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    cal_model.eval()\n",
        "    for tag in dataloader.tags:\n",
        "        test_x, test_y = dataloader.getTestDataSetTags(tag=tag)\n",
        "        test_x_torch = torch.from_numpy(test_x).type(torch.float).to(DEVICE)\n",
        "        pred = cal_model(test_x_torch).detach()\n",
        "        pred = pt_model.inverse_transform(pred).detach().to('cpu').numpy()\n",
        "        rmse = np.sqrt(MSE(test_y, pred))*1000\n",
        "        print(\" {} RMSE: {:0.2f}\".format(tag, rmse))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " W2 RMSE: 26.05\n",
            " W3 RMSE: 34.58\n",
            " W4 RMSE: 15.61\n",
            " W5 RMSE: 20.75\n",
            " W6 RMSE: 25.11\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}